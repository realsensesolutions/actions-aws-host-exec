---
name: 'Actions AWS Host Exec'
description: 'Execute scripts on EC2 instances using SSM with artifacts from S3'
inputs:
  name:
    description: 'Execution name'
    required: true
  artifacts:
    description: 'Path to folder containing artifacts to upload'
    required: true
  script:
    description: 'Path to script file within artifacts (relative to artifact root)'
    required: true
  targets:
    description: 'Target selection criteria in format KEY:VALUE (one per line). Example: Environment:DEV'
    required: true
  working-directory:
    description: 'Directory to extract artifacts and execute script in'
    required: false
    default: '/home/ssm-user'
  timeout:
    description: 'Execution timeout in seconds'
    required: false
    default: '3600'
  action:
    description: "Desired outcome: apply, plan or destroy"
    required: false
    default: "apply"

outputs:
  document:
    description: "SSM document ARN"
    value: ${{ steps.tf-outputs.outputs.document }}
  role_name:
    description: "IAM role name"
    value: ${{ steps.tf-outputs.outputs.role_name }}
  bucket:
    description: "S3 bucket name for artifacts"
    value: ${{ steps.tf-outputs.outputs.bucket }}

runs:
  using: "composite"

  steps:
    - name: init
      shell: bash
      working-directory: ${{ github.action_path }}
      run: |
        terraform init -reconfigure \
          -backend-config="bucket=${{ env.TF_BACKEND_s3 }}" \
          -backend-config="dynamodb_table=${{ env.TF_BACKEND_dynamodb }}" \
          -backend-config="key=${{ inputs.name }}"

    - name: run terraform action
      id: tf-action
      shell: bash
      working-directory: ${{ github.action_path }}
      env:
        ACTION: ${{ inputs.action }}
        ACTION_ARGS: ${{ inputs.action != 'plan' && '-auto-approve' || '' }}
        TF_VAR_name: ${{ inputs.name }}
        TF_VAR_working_directory: ${{ inputs.working-directory }}
        TF_VAR_timeout: ${{ inputs.timeout }}
        TF_VAR_targets: ${{ inputs.targets }}
        TF_VAR_script: ${{ inputs.script }}
        TF_VAR_artifacts_folder: "${{ github.workspace }}/${{ inputs.artifacts }}"
      run: terraform ${{ env.ACTION }} ${{ env.ACTION_ARGS }}

    - name: get terraform outputs
      id: tf-outputs
      if: inputs.action != 'destroy'
      shell: bash
      working-directory: ${{ github.action_path }}
      run: |
        DOCUMENT=$(terraform output -raw document)
        ROLE_NAME=$(terraform output -raw role_name)
        BUCKET=$(terraform output -raw bucket)
        echo "document=$DOCUMENT" >> $GITHUB_OUTPUT
        echo "role_name=$ROLE_NAME" >> $GITHUB_OUTPUT
        echo "bucket=$BUCKET" >> $GITHUB_OUTPUT

    - name: package and upload artifacts
      if: inputs.action != 'destroy'
      shell: bash
      run: |
        # Create tarball from artifacts folder
        ARTIFACTS_PATH="${{ github.workspace }}/${{ inputs.artifacts }}"
        ARTIFACT_FILE="/tmp/artifacts.tar.gz"
        
        echo "Packaging artifacts from $ARTIFACTS_PATH"
        cd "$ARTIFACTS_PATH"
        tar -czf "$ARTIFACT_FILE" .
        
        # Upload to S3
        BUCKET="${{ steps.tf-outputs.outputs.bucket }}"
        S3_KEY="${{ inputs.name }}/artifacts.tar.gz"
        
        echo "Uploading artifacts to s3://$BUCKET/$S3_KEY"
        aws s3 cp "$ARTIFACT_FILE" "s3://$BUCKET/$S3_KEY"
        
        echo "Artifacts uploaded successfully"
        rm -f "$ARTIFACT_FILE" 